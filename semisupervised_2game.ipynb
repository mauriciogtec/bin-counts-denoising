{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan import Generator, Discriminator\n",
    "from hierarchical_negbin import RecordGenerator\n",
    "from peak_detector7 import Features, SignalHead, DeconvHead\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import cm\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "cols = [\"#926cb6\", \"#93b793\", \"#d31d00\", \"#ff900d\", \"#fefb03\", \"black\"]\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXBINS = 400\n",
    "BATCHSIZE = 32  # max size is 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = RecordGenerator(n_obs=(1500, 100000),\n",
    "                            n_bins=(50, MAXBINS),\n",
    "                            n_meanings=(1, 3),\n",
    "                            noise_ratio=(0.1, 1.0),\n",
    "                            noise_dispersion=(0.05, 1.5),\n",
    "                            alpha_meanings=(1.0, 4.0),\n",
    "                            rounding=list(range(1, 17)),\n",
    "                            inner_mode_dist_tol=0.25,\n",
    "                            inner_sigma_ratio=3.0,\n",
    "                            max_sigma_to_bins_ratio=0.125,\n",
    "                            sigmas=(1.0, 50.0),\n",
    "                            trim_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geninput_noise = tf.keras.Input(shape=(MAXBINS, 1), dtype=tf.float32)\n",
    "geninput_signal = tf.keras.Input(shape=(MAXBINS, 1), dtype=tf.float32)\n",
    "geninput_noiseratio = tf.keras.Input(shape=(1, 1), dtype=tf.float32)\n",
    "disinput = tf.keras.Input(shape=(MAXBINS, 1), dtype=tf.float32)\n",
    "\n",
    "genoutput = Generator(ksize=5, filters=32, nblocks=16)([geninput_noise, geninput_signal, geninput_noiseratio])\n",
    "disoutput = Discriminator(ksize=7, filters=32, nblocks=16)(disinput)\n",
    "\n",
    "generator = tf.keras.Model(inputs=[geninput_noise, geninput_signal, geninput_noiseratio], outputs=genoutput)\n",
    "discriminator = tf.keras.Model(inputs=disinput, outputs=disoutput)\n",
    "\n",
    "# inputs_smoother = tf.keras.Input(shape=(MAXBINS, 1), dtype=tf.float32)\n",
    "# feats_smoother = Features(ksize=7, filters=32, nblocks=12)(inputs_smoother)\n",
    "# signal, peaks = SignalHead(ksize=5, filters=32, nblocks_signal=2, nblocks_peaks=4)([feats_smoother, inputs_smoother])\n",
    "# smoother = tf.keras.Model(inputs=inputs_smoother, outputs=[signal, peaks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights(\"generator.h5\")\n",
    "# discriminator.load_weights(\"discriminator.h5\")\n",
    "smoother.load_weights(\"tmp_back_to_two_smoother.h5\")\n",
    "# smoother.load_weights(\"smoother.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_bins(obs):\n",
    "    if len(obs) > MAXBINS:\n",
    "        start = (len(obs) - MAXBINS) // 2\n",
    "        obs = obs[start:(start + MAXBINS)]\n",
    "    L = len(obs)\n",
    "    assert L <= MAXBINS\n",
    "    start = MAXBINS // 2 - L // 2\n",
    "    end = start + L\n",
    "    out = np.zeros(MAXBINS, dtype=type(obs[0]))\n",
    "    out[start:end] = obs\n",
    "    return out, start\n",
    "\n",
    "    \n",
    "def preprocess_input(obs):\n",
    "    # assumes 1 vector of observation and possibly multiple peaks\n",
    "    obs, start = standardize_bins(obs)\n",
    "    x = np.array(obs, dtype=np.float32)\n",
    "    x /= x.sum()\n",
    "    xinput = x * np.sqrt(MAXBINS)\n",
    "    xinput = np.expand_dims(xinput, -1)\n",
    "    return xinput\n",
    "\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    # assumes 1 vector of observation and possibly multiple peaks\n",
    "    x = [preprocess_input(x) for x in batch]\n",
    "    x = np.stack(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_inputs(batch):\n",
    "    nbins_sizes = [len(x) for x in batch]\n",
    "    noises = []\n",
    "    signals = []\n",
    "    modes = []\n",
    "    for n in nbins_sizes:\n",
    "        z = np.random.standard_t(df=2, size=n)\n",
    "        z, _ = standardize_bins(z)\n",
    "        dat = simulator.generate(n_bins=n)\n",
    "        pdf = dat['pdf']\n",
    "        pdf, _ = standardize_bins(pdf)\n",
    "        m = dat['modes_onehot']\n",
    "        m, _ = standardize_bins(m)\n",
    "        modes.append(m)\n",
    "        noises.append(z)\n",
    "        signals.append(pdf)\n",
    "    noises = np.stack(noises, 0)       \n",
    "    noises = np.expand_dims(noises, -1)\n",
    "    signals = np.stack(signals, 0)       \n",
    "    signals = np.expand_dims(signals, -1)\n",
    "    modes = np.stack(modes, 0)       \n",
    "    modes = np.expand_dims(modes, -1)\n",
    "    nr = np.random.uniform(0.1, 0.5, size=(len(batch), 1, 1))\n",
    "    return noises, signals, nr, modes\n",
    "\n",
    "\n",
    "def get_batch(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        x = json.load(f)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_gan_inputs(batch):\n",
    "    batch = [x['counts'] for x in batch]\n",
    "    x = preprocess_batch(batch)\n",
    "    noises, pdfs, noiseratio, modes = get_inputs(batch)\n",
    "    return x, noises, pdfs, noiseratio, modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "# def smoother_loss(y, yhat):\n",
    "#     # symmetric kl\n",
    "#     x = - 0.5 * y * tf.math.log((yhat + 1e-10) / (y + 1e-10))\n",
    "#     x += - 0.5 * yhat * tf.math.log((y + 1e-10) / (yhat + 1e-10))\n",
    "#     x = tf.math.reduce_sum(x, 1)\n",
    "#     loss = tf.math.reduce_mean(x)\n",
    "#     return loss\n",
    "\n",
    "# def peaks_loss(y, yhat):\n",
    "#     # symmetric kl\n",
    "#     x = - 0.5 * y * tf.math.log((yhat + 1e-10) / (y + 1e-10))\n",
    "#     x += - 0.5 * yhat * tf.math.log((y + 1e-10) / (yhat + 1e-10))\n",
    "#     x = tf.math.reduce_sum(x, 1)\n",
    "#     loss = tf.math.reduce_mean(x)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(5e-5)\n",
    "# smoother_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "\n",
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(real_signal, noise, pdf, nr, modes):\n",
    "\n",
    "#     with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as sm_tape:\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise, pdf, nr], training=True)\n",
    "        real_output = discriminator(real_signal, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "#         smooth, peaks = smoother(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "#         sm_loss = smoother_loss(real_signal, smooth) + peaks_loss(modes, peaks)\n",
    "\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "#     gradients_of_smoother = sm_tape.gradient(sm_loss, smoother.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "#     smoother_optimizer.apply_gradients(zip(gradients_of_smoother, smoother.trainable_variables))\n",
    "    \n",
    "#     return gen_loss, disc_loss, sm_loss\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = \"../SmoothDoQ/doq_noun_batches/\"\n",
    "files = os.listdir(fdir)\n",
    "files = [os.path.join(fdir, x) for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch 1: =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-13f0fb67ce68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnr_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mmodes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoother_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# record losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-989dda53c0c9>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(real_signal, noise, pdf, nr, modes)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mgradients_of_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mgradients_of_discriminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mgradients_of_smoother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001b[1;32m-> 1181\u001b[1;33m       SmartBroadcastGradientArgs(x, y, grad))\n\u001b[0m\u001b[0;32m   1182\u001b[0m   \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36mSmartBroadcastGradientArgs\u001b[1;34m(x, y, grad)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0msx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0msy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_gradient_args\u001b[1;34m(s0, s1, name)\u001b[0m\n\u001b[0;32m    811\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m    812\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         \"BroadcastGradientArgs\", name, _ctx._post_execution_callbacks, s0, s1)\n\u001b[0m\u001b[0;32m    814\u001b[0m       \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_BroadcastGradientArgsOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 5\n",
    "plot_every = 50\n",
    "save_every = 250\n",
    "lam = 0.01\n",
    "epochs = 10\n",
    "entries_per_file = 64\n",
    "batches_per_file = entries_per_file // BATCHSIZE\n",
    "np.random.shuffle(files)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"===== Epoch 1: =====\")\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        file_data = get_batch(file)\n",
    "        for b in range(batches_per_file):\n",
    "            batch = file_data[b*BATCHSIZE:(b + 1)*BATCHSIZE]\n",
    "            x, z, s, nr, modes = get_gan_inputs(batch)\n",
    "            x_ = tf.constant(x, tf.float32)\n",
    "            z_ = tf.constant(z, tf.float32)\n",
    "            s_ = tf.constant(s, tf.float32)\n",
    "            nr_ = tf.constant(nr, tf.float32)\n",
    "            modes_ = tf.constant(modes, tf.float32)\n",
    "#             gen_loss, disc_loss, smoother_loss = train_step(x_, z_, s_, nr_, modes_)\n",
    "            gen_loss, disc_loss = train_step(x_, z_, s_, nr_, modes_)\n",
    "\n",
    "            # record losses\n",
    "            if not first_run:\n",
    "                gen_loss_ += lam * (gen_loss.numpy() - gen_loss_)\n",
    "                disc_loss_ += lam * (disc_loss.numpy() - disc_loss_)\n",
    "#                 sm_loss_ += lam * (sm_loss_.numpy() - sm_loss_)\n",
    "                first_run = False\n",
    "            else:\n",
    "                gen_loss_ = gen_loss.numpy()\n",
    "                disc_loss_ = disc_loss.numpy()\n",
    "#                 sm_loss_ = sm_loss_.numpy()\n",
    "                \n",
    "            # print\n",
    "            if i % print_every == 0:\n",
    "                fr = f\"iter: {i}, completed: {(i + 1) / len(files):.2f}%\"\n",
    "                gl = f\"gen_loss: {gen_loss_:.3f}\"\n",
    "                dl = f\"disc_loss: {disc_loss_:.3f}\"\n",
    "#                 sl = f\"smoother_loss: {smoother_loss_:.3f}\"\n",
    "#                 msg = f\"{fr}, {gl}, {dl}, {sl}\"\n",
    "                msg = f\"{fr}, {gl}, {dl}\"\n",
    "                print(msg)\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                x0 = np.expand_dims(x[0, :, :], 0).astype(np.float32)\n",
    "                z0 = np.expand_dims(z[0, :, :], 0).astype(np.float32)\n",
    "                s0 = np.expand_dims(s[0, :, :], 0).astype(np.float32)\n",
    "                nr0 = np.expand_dims(nr[0, :, :], 0).astype(np.float32)\n",
    "                pdf = np.squeeze(s0)            \n",
    "                fake = generator([z0, s0, nr0])\n",
    "                fake = np.squeeze(fake.numpy())\n",
    "                in_range = np.where(pdf > 0.0)[0]\n",
    "                h = range(in_range[0], in_range[-1])    \n",
    "                plt.figure(figsize=(15, 4))\n",
    "                plt.bar(h, fake[h], width=1.0, alpha=0.75)\n",
    "                plt.plot(h, pdf[h], c=\"red\")\n",
    "                u = pdf[h]\n",
    "                plt.ylim(0, 25 * np.mean(u[u > 0]))\n",
    "                plt.title(f\"noise ratio: {nr0[0,0,0]:.2f}\")\n",
    "                plt.show()\n",
    "\n",
    "            # Save the model every 15 epochs\n",
    "            if (i + 1) % save_every == 0:\n",
    "                generator.save_weights(\"generator.h5\")\n",
    "                discriminator.save_weights(\"discriminator.h5\")\n",
    "#                 smoother.save_weights(\"smoother.h5\")\n",
    "                \n",
    "                \n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
